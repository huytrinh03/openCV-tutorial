{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296710d5-e9da-46cd-a1a7-dfc84d8c9496",
   "metadata": {},
   "source": [
    "## Installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14fb294-36e4-49ff-9847-bcf0c16c00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5942ba-d4ed-4f76-9f0c-18dbd70f6906",
   "metadata": {},
   "source": [
    "## Converting an image to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc12fa6-db01-42b3-8eac-27420ec36dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 10:42:02.643 Python[19885:14661199] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the input image and display the image to our screen\n",
    "image = cv2.imread(\"image2.png\")\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)\n",
    "# convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(\"Gray\", gray)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05590c-1300-4a58-9cad-2fa101942344",
   "metadata": {},
   "source": [
    "## Edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ff7abc-b564-442d-a1a5-1456127aa698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying edge detection we can find the outlines of objects in images\n",
    "edged = cv2.Canny(gray, 10, 180)\n",
    "cv2.imshow(\"Edged\", edged)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a0c19-1fd1-4368-8bfa-a0120fee503f",
   "metadata": {},
   "source": [
    "## Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c80fe8c-34b3-4fc8-90b7-0e4b6dc09646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# threshold the image by setting all pixel values less than 225\n",
    "# to 0 (white; foreground) and all pixel values >= 225 to 255\n",
    "# (black; background), thereby segmenting the image\n",
    "threshold = cv2.threshold(gray, 230, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "cv2.imshow(\"Threshold\", threshold)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4430022f-32c9-4850-a009-2fef4f7bccb4",
   "metadata": {},
   "source": [
    "## Finding contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53924e4e-adb2-46ac-96b5-aa5bebd04866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find contours (i.e., outlines) of the foreground objects in the\n",
    "# thresholded image\n",
    "cnts = cv2.findContours(threshold.copy(), cv2.RETR_EXTERNAL,\n",
    "\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "output = image.copy()\n",
    "# loop over the contours\n",
    "for c in cnts:\n",
    "\t# draw each contour on the output image with a 3px thick purple\n",
    "\t# outline, then display the output contours one at a time\n",
    "\tcv2.drawContours(output, [c], -1, (240, 0, 159), 3)\n",
    "\tcv2.imshow(\"Contours\", output)\n",
    "\tcv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29198dca-f067-4d55-a17c-0e665901f22f",
   "metadata": {},
   "source": [
    "## Count the number of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b13f1738-7512-414b-9975-e46307703aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw the total number of contours found in purple\n",
    "text = \"I found {} objects!\".format(len(cnts))\n",
    "cv2.putText(output, text, (10, 25),  cv2.FONT_HERSHEY_SIMPLEX, 0.7,\n",
    "\t(240, 0, 159), 2)\n",
    "cv2.imshow(\"Contours\", output)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651da56d-15d0-4416-8011-aa6bf78693d4",
   "metadata": {},
   "source": [
    "## Erosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9df7d-02dc-4e37-a50f-f2166827f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we apply erosions to reduce the size of foreground objects\n",
    "mask = threshold.copy()\n",
    "mask = cv2.erode(mask, None, iterations=5)\n",
    "cv2.imshow(\"Eroded\", mask)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd01016-f167-4ff8-9905-474716d9ba63",
   "metadata": {},
   "source": [
    "## Dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c08b1-8e2a-483a-9248-c7ee9a15ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly, dilations can increase the size of the ground objects\n",
    "# Can be used to connect nearby contours\n",
    "mask = threshold.copy()\n",
    "mask = cv2.dilate(mask, None, iterations=5)\n",
    "cv2.imshow(\"Dilated\", mask)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b53fd8-51c7-4ed6-b5c6-b544f3f0b548",
   "metadata": {},
   "source": [
    "## Bit-wise masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b49d57-f8ca-44ad-a808-156b84491f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a typical operation we may want to apply is to take our mask and\n",
    "# apply a bitwise AND to our input image, keeping only the masked\n",
    "# regions\n",
    "mask = threshold.copy()\n",
    "output = cv2.bitwise_and(image, image, mask=mask)\n",
    "cv2.imshow(\"Output\", output)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a7bfcb-3836-4863-961b-f7c0ef43be7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openCV-tutorial-venv",
   "language": "python",
   "name": "opencv-tutorial-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
